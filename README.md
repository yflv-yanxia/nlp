# nlp

## RE
[std::regex/Boost.Regex-c++]<br>
[hyperscan-c++/python], a large number of regular expressions, only for x86<br>
[QRegExp-c++]<br>
[re-python]<br>
[PCRE/PCRE++-perl/c++]<br>
[google/re2-c++/go/python], a large number of regular expressions<br>
[comparision](https://en.wikipedia.org/wiki/Comparison_of_regular-expression_engines)<br>

## LAC
[Chinese Lexical Analysis with Deep Bi-GRU-CRF Network](https://arxiv.org/abs/1807.01882v1) -baidu, arxiv2018<br>

### pretrain models
[thulac]<br>
[baidu/lac]<br>
[HIT-SCIR/ltp](https://github.com/HIT-SCIR/ltp)<br>
[spacy](https://spacy.io/)<br>
[stanza](https://stanfordnlp.github.io/stanza/)<br>
[hanlp]<br>


## Machine Reading Comprehension
[Improving Machine Reading Comprehension with Single-choice Decision and Transfer Learning](https://arxiv.org/abs/2011.03292) -tencent, arxiv2020<br>
[DUMA: Reading Comprehension with Transposition Thinking](https://arxiv.org/abs/2001.09415) -huawei, arxiv2020<br>
[DCMN+: Dual co-matching network for multi-choice reading comprehension](https://ojs.aaai.org/index.php/AAAI/article/view/6502) -cloudwalk, AAAI2020<br>
[Albert: A lite bert for self-supervised learning of language representations](https://arxiv.org/abs/1909.11942) -google, ICLR2020<br>
[Dual co-matching network for multi-choice reading comprehension](https://arxiv.org/abs/1901.09381) -cloudwalk, arxiv2019<br>
[Option comparison network for multiple-choice reading comprehension](https://arxiv.org/abs/1903.03033) -tencent, arxiv2019<br>
[Neural Machine Reading Comprehension: Methods and Trends](https://www.mdpi.com/2076-3417/9/18/3698) -S Liu, AppliedSciences2019<br>
[Applying deep learning to answer selection: A study and an open task](https://ieeexplore.ieee.org/abstract/document/7404872) -IBM, ASRU2015<br>

### databse
[DREAM](https://dataset.org/dream/)<br>
[RACE](http://www.qizhexie.com/data/RACE_leaderboard.html)<br>
[SQuAD2.0]<br>
[ARC]<br>
[CoQA]<br>

## NER
[A survey on deep learning for named entity recognition](https://arxiv.org/abs/1812.09449) -TKDE2020<br>

### database
[Ontonotes release 4.0/5.0](https://catalog.ldc.upenn.edu/LDC2013T19)<br>
[MSRA, Word segmentation and named entity recognition](https://huggingface.co/datasets/viewer/?dataset=msra_ner)<br>
[Weibo NER, recognition for Chinese social media with jointly trained embeddings](https://huggingface.co/datasets/viewer/?dataset=weibo_ner)<br>
[人民日报](https://huggingface.co/datasets/viewer/?dataset=peoples_daily_ner)<br>
[BosonNLP_NER_6C](https://bosonnlp.com/resources/BosonNLP_NER_6C.zip), [bosonnlp](http://docs.bosonnlp.com/ner.html)<br>
[CCKS2017/2018/2019/2020电子病历实体标注](https://www.biendata.com/competition/ccks_2020_2_1/)<br>
[WikiANN/PAN-X](https://huggingface.co/datasets/viewer/?dataset=wikiann)<br>
[XGLUE](https://huggingface.co/datasets/viewer/?dataset=xglue)<br>
[CLUENER2020](https://github.com/CLUEbenchmark/CLUENER2020)<br>

### pretrain models
[baidu/ERNIE](https://github.com/PaddlePaddle/ERNIE)<br>
[baidu/lac](https://github.com/baidu/lac)<br>
[HIT-SCIR/ltp](https://github.com/HIT-SCIR/ltp)<br>
[spacy](https://spacy.io/)<br>
[stanza](https://stanfordnlp.github.io/stanza/)<br>
[腾讯UER](https://github.com/dbiir/UER-py/blob/master/README_ZH.md)<br>
[CLUEPretrainedModels](https://github.com/CLUEbenchmark/CLUEPretrainedModels)<br>
[Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)<br>
[google](https://github.com/google-research/bert)<br>

## Dependency Parsing
[Efficient Second-Order TreeCRF for Neural Dependency Parsing](https://www.aclweb.org/anthology/2020.acl-main.302/) -SoochowUniversity, ACL2020, [code](https://github.com/yzhangcs/crfpar)<br>
[Deep Biaffine Attention for Neural Dependency Parsing](https://arxiv.org/abs/1611.01734) -Stanford, ICLR2017<br>

### database

### pretrain models
[baidu/DDParser](https://github.com/baidu/DDParser)<br>
[HIT-SCIR/ltp](https://github.com/HIT-SCIR/ltp)<br>
[spacy](https://spacy.io/)<br>
[stanza](https://stanfordnlp.github.io/stanza/)<br>

## Post Editing
[A survey on non-autoregressive generation for neural machine translation and beyond](https://ieeexplore.ieee.org/abstract/document/10129160) -msra, PAMI2023, [linker](https://github.com/LitterBrother-Xiao/Overview-of-Non-autoregressive-Applications)<br>
[Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation](https://arxiv.org/abs/2304.11791) -tsinghua, ACL2023, [code](https://github.com/thu-coai/DA-Transformer)<br>
[Directed Acyclic Transformer for Non-Autoregressive Machine Translation](https://arxiv.org/abs/2205.07459) -bytedance, ICML2022<br>
[Hierarchical Context Tagging for Utterance Rewriting](https://ojs.aaai.org/index.php/AAAI/article/view/21331) -tencent, AAAI2022<br>
[Text generation with text-editing models](https://arxiv.org/abs/2206.07043) -NAACL2022<br>
[EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start](https://arxiv.org/abs/2205.12209) -google, EMNLP2022, [code](https://edit5.page.link/code)<br>
[LEWIS: Levenshtein Editing for Unsupervised Text Style Transfer](https://aclanthology.org/2021.findings-acl.344/) -ACL2021<br>
[LayoutReader: Pre-training of Text and Layout for Reading Order Detection](https://arxiv.org/abs/2108.11591) -EMNLP2021, [code&dataset](https://github.com/microsoft/unilm/tree/master/layoutreader)<br>
[Softcorrect: Error correction with soft detection for automatic speech recognition](https://ojs.aaai.org/index.php/AAAI/article/view/26531) -microsoft, AAAI2023<br>
[FastCorrect2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition](https://aclanthology.org/2021.findings-emnlp.367/) -microsoft, EMNLP2021, [code](https://github.com/microsoft/NeuralSpeech)<br>
[FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition](https://arxiv.org/abs/2105.03842) -microsoft, NeurIPS2021, [code](https://github.com/microsoft/NeuralSpeech)<br>
[FELIX: Flexible Text Editing Through Tagging and Insertion](https://www.aclweb.org/anthology/2020.findings-emnlp.111/) -google, EMNLP2020, [code](https://github.com/google-research/google-research/tree/master/felix)<br>
[Seq2Edits: Sequence transduction using span-level edit operations](https://arxiv.org/abs/2009.11136) -google, EMNLP2020<br>
[Spelling Error Correction with Soft-Masked BERT](https://arxiv.org/abs/2005.07421) -bytedance, arxiv2020<br>
[SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check](https://arxiv.org/abs/2004.14166) -alibaba, ACL2020<br>
[Encode, Tag, Realize: High-Precision Text Editing](https://arxiv.org/abs/1909.01187) -google, EMNLP2019<br>
[Levenshtein transformer](https://arxiv.org/abs/1905.11006v1) -facebook, NIPS2019<br>
[Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/abs/1905.03197) -NIPS2019<br>
[A spelling correction model for end-to-end speech recognition](https://arxiv.org/abs/1902.07178) -google, ICASSP 2019<br>
[Automatic Spelling Correction with Transformer for CTC-based End-to-End Speech Recognition](https://arxiv.org/abs/1904.10045) -alibaba, arxiv2019<br>

### dict
CLUECorpus2020
Google原始中文词表


### pretrain 
[CLUECorpus2020](https://github.com/CLUEbenchmark/CLUECorpus2020)<br>
[brightmart](https://github.com/brightmart/nlp_chinese_corpus)<br>
人民日报1998版本<br>
人民日报2014版本<br>


### CLUE
[中文医疗信息处理挑战榜CBLUE](https://tianchi.aliyun.com/specials/promotion/2021chinesemedicalnlpleaderboardchallenge), [database](https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414)<br>


### QA
#### english
CLUE benchmark
google, Natural Questions: a Benchmark for Question Answering Research

#### chinese
哈工大、讯飞CMRC
DRCD

### cls
CLUE benchmark
清华大学开源的文本分类数据集THUCTC<br>




## labeling tools
YEDDA<br>






